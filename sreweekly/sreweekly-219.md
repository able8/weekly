## [SRE Weekly Issue #219](https://sreweekly.com/sre-weekly-issue-219/) - May 17, 2020
### Articles

1. [Download our new on-call book [Atlassian]](https://www.atlassian.com/blog/it-teams/on-call-book)

    Check out this new 100-page ebook on incident response from Atlassian, great for folks setting up a brand new on-call structure or improving their existing one. It even has a section on compensating teams for being on-call.Serhat Can — Atlassian
1. [How Many Is Too Much? Exploring Costs of Coordination During Outages ](https://www.infoq.com/presentations/incident-command-system/)

    Laura Maguire discusses the compelling data from her PhD dissertation that the Incident Command System actually makes incident response less efficient, along with lots of other interesting findings.Laura Maguire
1. [“The Future of DevOps is Resilience Engineering” Incident Retrospective](https://www.blameless.com/amy-tobeys-future-of-devops-resilience-engineering-2/)

    A summary of a great talk by Amy Tobey at Failover Conf, amusingly framed as a “retrospective”.Hannah Culver — Blameless
1. [Operations in the Cloud](https://billduncan.org/operations-in-the-cloud/)

    In this case, the “cloud” refers to actual clouds, the ones in the sky. It’s a comparison between concepts in aviation and SRE, fields that have significant overlaps.Bill Duncan
1. [Five causes of poor availability to watch out for](https://diginomica.com/five-causes-poor-availability-watch-out)

    My favorite:Lee Atchison — diginomica
1. [How a Facebook Bug Took Down Spotify, TikTok, and Other Major iOS Apps](https://www.wired.com/story/facebook-sdk-ios-apps-spotify-tiktok-crash/)

    A bug in a new release of the Facebook SDK caused some iOS apps to crash.Brian Barrett — WIRED
1. [Making peace with “root cause” during anomaly response](https://surfingcomplexity.blog/2020/05/13/making-peace-with-root-cause-during-anomaly-response/)

    Loren Hochstein
### Outages

1. [Slack](https://status.slack.com/2020-05/147dad376c8946ff)
    Slack’s server infrastructure scales up every day to handle volume in North America by increasing the size of the server pool available to handle requests. Some of these servers did not successfully register with our load balancing infrastructure during this process of scaling up, and this ultimately led to a decline in the health of the server pool over time.
1. [Youtube](https://www.animatedtimes.com/youtube-outage-freaked-the-internet/)
1. [Coinbase](https://invezz.com/news/2020/05/10/ethereum-price-crashes-10-lower-as-coinbase-reports-system-outage/)
1. [Google Play Store](https://9to5google.com/2020/05/13/google-play-store-outage/)
1. [Microsoft Outlook](https://www.mediapost.com/publications/article/351382/outlook-goes-down-briefly-on-wednesday-afternoon.html)
1. [reddit](https://reddit.statuspage.io/incidents/0qzvnzbj7h05)
1. [Zoom](https://status.zoom.us/incidents/hc2dk68vhjyl)

### [ << Prev ](sreweekly-218.md) ------------- [ Next >> ](sreweekly-220.md)